{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\theot\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset_builder\n",
    "import datasets\n",
    "\n",
    "ds_builder = load_dataset_builder(\"imdb\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dataset\n",
    "\n",
    "How many split does the dataset has ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data set has 3 splits !\n"
     ]
    }
   ],
   "source": [
    "nb_splits = len(ds_builder.info.splits)\n",
    "print(\"The data set has {} splits !\".format(nb_splits))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How big are these splits ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': SplitInfo(name='train', num_bytes=33432823, num_examples=25000, shard_lengths=None, dataset_name='imdb'),\n",
       " 'test': SplitInfo(name='test', num_bytes=32650685, num_examples=25000, shard_lengths=None, dataset_name='imdb'),\n",
       " 'unsupervised': SplitInfo(name='unsupervised', num_bytes=67106794, num_examples=50000, shard_lengths=None, dataset_name='imdb')}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_builder.info.splits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of train split is 25000\n",
    "The size of test split is 25000\n",
    "The size of unsupervised split is 50000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the proportion of each class on the supervised splits ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (C:/Users/theot/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n",
      "100%|██████████| 3/3 [00:00<00:00, 10.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': Value(dtype='string', id=None), 'label': ClassLabel(names=['neg', 'pos'], id=None)}\n",
      "{'text': Value(dtype='string', id=None), 'label': ClassLabel(names=['neg', 'pos'], id=None)}\n",
      "Train size: 25000 - Train pos: 12500 - Train neg: 12500\n",
      "Test size: 25000 - Test pos: 12500 - Test neg: 12500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.load_dataset('imdb')\n",
    "\n",
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']\n",
    "\n",
    "train_size = len(train_dataset)\n",
    "test_size = len(test_dataset)\n",
    "\n",
    "train_pos = sum(train_dataset['label'])\n",
    "train_neg = train_size - train_pos\n",
    "test_pos = sum(test_dataset['label'])\n",
    "test_neg = test_size - test_pos\n",
    "\n",
    "# Print the results\n",
    "print(train_dataset.features)\n",
    "print(test_dataset.features)\n",
    "print('Train size: {} - Train pos: {} - Train neg: {}'.format(train_size, train_pos, train_neg))\n",
    "print('Test size: {} - Test pos: {} - Test neg: {}'.format(test_size, test_pos, test_neg))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the dataset contains the column \"text\" (that contains string) and the colums \"label\" (that contains class \"neg\" or \"pos\") that indicate if it's a positive or a negative comment\n",
    "\n",
    "Train split contains 12500 pos (50%) and 12500 neg (50%)\n",
    "Test split contains 12500 pos (50%) and 12500 neg (50%)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NAIVE BAYES CLASSIFIER\n",
    "Implement your own naive Bayes classifier (the pseudo code can be found in the slides or the book reference) or use one provided by scikit-learn combined with a CountVectorizer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lower case the text and replace punctuation with space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i rented i am curious-yellow from my video store because of all the controversy that surrounded it when it was first released in 1967  i also heard that at first it was seized by u s  customs if it ever tried to enter this country  therefore being a fan of films considered  controversial  i really had to see this for myself the plot is centered around a young swedish drama student named lena who wants to learn everything she can about life  in particular she wants to focus her attentions to making some sort of documentary on what the average swede thought about certain political issues such as the vietnam war and race issues in the united states  in between asking politicians and ordinary denizens of stockholm about their opinions on politics  she has sex with her drama teacher  classmates  and married men what kills me about i am curious-yellow is that 40 years ago  this was considered pornographic  really  the sex and nudity scenes are few and far between  even then it's not shot like some cheaply made porno  while my countrymen mind find it shocking  in reality sex and nudity are a major staple in swedish cinema  even ingmar bergman  arguably their answer to good old boy john ford  had sex scenes in his films i do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in america  i am curious-yellow is a good film for anyone wanting to study the meat and potatoes  no pun intended  of swedish cinema  but really  this film doesn't have much of a plot \n",
      " i am curious  yellow  is a risible and pretentious steaming pile  it doesn't matter what one's political views are because this film can hardly be taken seriously on any level  as for the claim that frontal male nudity is an automatic nc-17  that isn't true  i've seen r-rated films with male nudity  granted  they only offer some fleeting views  but where are the r-rated films with gaping vulvas and flapping labia  nowhere  because they don't exist  the same goes for those crappy cable shows  schlongs swinging in the breeze but not a clitoris in sight  and those pretentious indie movies like the brown bunny  in which we're treated to the site of vincent gallo's throbbing johnson  but not a trace of pink visible on chloe sevigny  before crying  or implying   double-standard  in matters of nudity  the mentally obtuse should take into account one unavoidably obvious anatomical difference between men and women  there are no genitals on display when actresses appears nude  and the same cannot be said for a man  in fact  you generally won't see female genitals in an american film in anything short of porn or explicit erotica  this alleged double-standard is less a double standard than an admittedly depressing ability to come to terms culturally with the insides of women's bodies \n",
      "i love sci-fi and am willing to put up with a lot  sci-fi movies tv are usually underfunded  under-appreciated and misunderstood  i tried to like this  i really did  but it is to good tv sci-fi as babylon 5 is to star trek  the original   silly prosthetics  cheap cardboard sets  stilted dialogues  cg that doesn't match the background  and painfully one-dimensional characters cannot be overcome with a 'sci-fi' setting   i'm sure there are those of you out there who think babylon 5 is good sci-fi tv  it's not  it's clichéd and uninspiring   while us viewers might like emotion and character development  sci-fi is a genre that does not take itself seriously  cf  star trek   it may treat important issues  yet not as a serious philosophy  it's really difficult to care about the characters here as they are not simply foolish  just missing a spark of life  their actions and reactions are wooden and predictable  often painful to watch  the makers of earth know it's rubbish as they have to always say  gene roddenberry's earth     otherwise people would not continue watching  roddenberry's ashes must be turning in their orbit as this dull  cheap  poorly edited  watching it without advert breaks really brings this home  trudging trabant of a show lumbers into space  spoiler  so  kill off a main character  and then bring him back as another actor  jeeez  dallas all over again \n",
      "worth the entertainment value of a rental  especially if you like action movies  this one features the usual car chases  fights with the great van damme kick style  shooting battles with the 40 shell load shotgun  and even terrorist style bombs  all of this is entertaining and competently handled but there is nothing that really blows you away if you've seen your share before the plot is made interesting by the inclusion of a rabbit  which is clever but hardly profound  many of the characters are heavily stereotyped -- the angry veterans  the terrified illegal aliens  the crooked cops  the indifferent feds  the bitchy tough lady station head  the crooked politician  the fat federale who looks like he was typecast as the mexican in a hollywood movie from the 1940s  all passably acted but again nothing special i thought the main villains were pretty well done and fairly well acted  by the end of the movie you certainly knew who the good guys were and weren't  there was an emotional lift as the really bad ones got their just deserts  very simplistic  but then you weren't expecting hamlet  right  the only thing i found really annoying was the constant cuts to vds daughter during the last fight scene not bad  not good  passable 4 \n",
      "this is just a precious little diamond  the play  the script are excellent  i cant compare this movie with anything else  maybe except the movie  leon  wonderfully played by jean reno and natalie portman  but    what can i say about this one  this is the best movie anne parillaud has ever played in  see please  frankie starlight   she's speaking english there  to see what i mean  the story of young punk girl nikita  taken into the depraved world of the secret government forces has been exceptionally over used by americans  never mind the  point of no return  and especially the  la femme nikita  tv series  they cannot compare the original believe me  trash these videos  buy this one  do not rent it  buy it  btw beware of the subtitles of the la company which  translate  the us release  what a disgrace  if you cant understand french  get a dubbed version  but you'll regret later   \n",
      "when i say this is my favourite film of all time  that comment is not to be taken lightly  i probably watch far too many films than is healthy for me  and have loved quite a few of them  i first saw  la femme nikita  nearly ten years ago  and it still manages to be my absolute favourite  why this is more than an incredibly stylish and sexy thriller  luc besson's great flair for impeccable direction  fashion  and appropriate usage of music makes this a very watchable film  but it is anne parillaud's perfect rendering of a complex character who transforms from a heartless killer into a compassionate  vibrant young woman that makes this film beautiful  i can't keep my eyes off of her when she is on screen i have seen several of luc besson's films including  subway    the professional   and the irritating  fifth element   and  nikita  is without a doubt  far superior to any of these  although this film has tragic elements  it is ultimately extremely hopeful  it is the story of a person who is cruel and merciless  who ultimately comes to realize her own humanity and her own personal power  that  to me is extremely inspiring  if there is hope for nikita  there is hope for all of us \n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "\n",
    "for split in dataset.keys():\n",
    "    i = 0\n",
    "    for text in dataset[split]['text']:\n",
    "        text = text.lower()\n",
    "        # remove <br /> tags\n",
    "        text = text.replace('<br />', '')\n",
    "        # remove punctuation\n",
    "        text = ''.join([c if c not in \"!\\\"#$%&()*+,./:;<=>?@[\\]^_`{|}~\" else ' ' for c in text])\n",
    "        print(text)\n",
    "        i += 1\n",
    "        if (i == 2):\n",
    "            break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement your own naive Bayes classifier from scratch. The pseudo code can be found in the slides or the book reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def lowercase_punctuation(text):\n",
    "    newtext = text.lower()\n",
    "    # remove <br /> tags\n",
    "    newtext = newtext.replace('<br />', '')\n",
    "    # remove punctuation\n",
    "    newtext = ''.join([c if c not in \"!\\\"#$%&()*+,./:;<=>?@[\\]^_`{|}~\" else ' ' for c in newtext])\n",
    "    return newtext\n",
    "\n",
    "def get_all_words(dataset):\n",
    "    all_words = []\n",
    "    for obj in dataset:\n",
    "        all_words += lowercase_punctuation(obj[\"text\"]).split()\n",
    "    #remove double\n",
    "    all_words = list(set(all_words))\n",
    "    return all_words\n",
    "\n",
    "def bigdoc(dataset, feature):\n",
    "    bigdoc = []\n",
    "    for obj in dataset:\n",
    "        if (obj[\"label\"] == feature):\n",
    "            bigdoc += lowercase_punctuation(obj[\"text\"]).split()\n",
    "    return bigdoc\n",
    "\n",
    "def train_naive_bayes_classifier_scratch(dataset, features):\n",
    "    words = get_all_words(dataset)\n",
    "    nbwords = len(words)\n",
    "    logprior = np.zeros(len(features))\n",
    "    loglikelihood = np.zeros((len(features), len(words)))\n",
    "    ndoc = len(dataset)\n",
    "    for feature in features:\n",
    "        nc = dataset[\"label\"].count(feature)\n",
    "        logprior[feature] = np.log(nc/ndoc)\n",
    "        bigd = bigdoc(dataset, feature)\n",
    "        lgbigd = len(bigd)\n",
    "        count = Counter(bigd)\n",
    "        for i, word in enumerate(words):\n",
    "            c = count[word]\n",
    "            loglikelihood[feature, i] = np.log((c + 1)/(lgbigd + nbwords))\n",
    "    return logprior, loglikelihood, words\n",
    "\n",
    "\n",
    "train_dataset = dataset['train']\n",
    "logprior, loglikelihood, words = train_naive_bayes_classifier_scratch(train_dataset, [0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.81504\n"
     ]
    }
   ],
   "source": [
    "def predict_string(test_string, logprior, loglikelihood, C, vocab_dict):\n",
    "    sum = np.zeros(len(C))\n",
    "    for c in C:\n",
    "        sum[c] = logprior[c]\n",
    "        target_word_list = lowercase_punctuation(test_string).split()\n",
    "        for word in target_word_list:\n",
    "            if word in vocab_dict:\n",
    "                sum[c] = sum[c] + loglikelihood[c, vocab_dict[word]]\n",
    "    return np.argmax(sum)\n",
    "\n",
    "def test_naive_bayes_classifier_scratch(test_dataset, logprior, loglikelihood, C, V):\n",
    "    word_dict = {word: i for i, word in enumerate(V)}\n",
    "    total = len(test_dataset)\n",
    "    correct = 0\n",
    "    for obj in test_dataset:\n",
    "        pred = predict_string(obj[\"text\"], logprior, loglikelihood, C, word_dict)\n",
    "        if pred == obj[\"label\"]:\n",
    "            correct += 1\n",
    "    print(\"Accuracy: {}\".format(correct/total))\n",
    "\n",
    "test_dataset = dataset['test']\n",
    "test_naive_bayes_classifier_scratch(test_dataset, logprior, loglikelihood, [0, 1], words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "train_dataset = dataset['train']\n",
    "\n",
    "def train_naive_bayes_classifier_scikit(train_dataset):\n",
    "    # Créer une instance de CountVectorizer pour convertir les textes en vecteurs de compte\n",
    "    vectorizer = CountVectorizer(lowercase=True, stop_words=\"english\")\n",
    "    # Vectoriser les textes d'entraînement\n",
    "    X_train = vectorizer.fit_transform(train_dataset[\"text\"])\n",
    "    # Créer un modèle de classification Naive Bayes multinomial\n",
    "    model = MultinomialNB()\n",
    "    # Entraîner le modèle\n",
    "    model.fit(X_train, train_dataset[\"label\"])\n",
    "    # Créer un pipeline qui vectorise et classe les textes en une seule étape\n",
    "    pipeline = make_pipeline(vectorizer, model)\n",
    "    # Retourner le pipeline entraîné\n",
    "    return pipeline\n",
    "\n",
    "pipeline = train_naive_bayes_classifier_scikit(train_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.81968\n"
     ]
    }
   ],
   "source": [
    "def test_naive_bayes_classifier_scikit(test_dataset, pipeline):\n",
    "    correct = 0\n",
    "    total = len(test_dataset)\n",
    "    for obj in test_dataset:\n",
    "        pred = pipeline.predict([obj[\"text\"]])\n",
    "        if pred == obj[\"label\"]:\n",
    "            correct += 1\n",
    "    print(\"Accuracy: {}\".format(correct/total))\n",
    "\n",
    "test_dataset = dataset['test']\n",
    "test_naive_bayes_classifier_scikit(test_dataset, pipeline)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"COMAPRER LES RESULTATS\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Isaac Florentine has made some of the best western Martial Arts action movies ever produced. In particular US Seals 2, Cold Harvest, Special Forces and Undisputed 2 are all action classics. You can tell Isaac has a real passion for the genre and his films are always eventful, creative and sharp affairs, with some of the best fight sequences an action fan could hope for. In particular he has found a muse with Scott Adkins, as talented an actor and action performer as you could hope for. This is borne out with Special Forces and Undisputed 2, but unfortunately The Shepherd just doesn't live up to their abilities.<br /><br />There is no doubt that JCVD looks better here fight-wise than he has done in years, especially in the fight he has (for pretty much no reason) in a prison cell, and in the final showdown with Scott, but look in his eyes. JCVD seems to be dead inside. There's nothing in his eyes at all. It's like he just doesn't care about anything throughout the whole film. And this is the leading man.<br /><br />There are other dodgy aspects to the film, script-wise and visually, but the main problem is that you are utterly unable to empathise with the hero of the film. A genuine shame as I know we all wanted this film to be as special as it genuinely could have been. There are some good bits, mostly the action scenes themselves. This film had a terrific director and action choreographer, and an awesome opponent for JCVD to face down. This could have been the one to bring the veteran action star back up to scratch in the balls-out action movie stakes.<br /><br />Sincerely a shame that this didn't happen.\n",
      "Predicted: 1\n",
      "Actual: 0\n",
      "\n",
      "Text: Blind Date (Columbia Pictures, 1934), was a decent film, but I have a few issues with this film. First of all, I don't fault the actors in this film at all, but more or less, I have a problem with the script. Also, I understand that this film was made in the 1930's and people were looking to escape reality, but the script made Ann Sothern's character look weak. She kept going back and forth between suitors and I felt as though she should have stayed with Paul Kelly's character in the end. He truly did care about her and her family and would have done anything for her and he did by giving her up in the end to fickle Neil Hamilton who in my opinion was only out for a good time. Paul Kelly's character, although a workaholic was a man of integrity and truly loved Kitty (Ann Sothern) as opposed to Neil Hamilton, while he did like her a lot, I didn't see the depth of love that he had for her character. The production values were great, but the script could have used a little work.\n",
      "Predicted: 1\n",
      "Actual: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def find_wrong_prediction_scratch(test_dataset, logprior, loglikelihood, C, V):\n",
    "    i = 0\n",
    "    word_dict = {word: i for i, word in enumerate(V)}\n",
    "    for obj in test_dataset:\n",
    "        pred = predict_string(obj[\"text\"], logprior, loglikelihood, C, word_dict)\n",
    "        if pred != obj[\"label\"]:\n",
    "            print(\"Text: {}\".format(obj[\"text\"]))\n",
    "            print(\"Predicted: {}\".format(pred))\n",
    "            print(\"Actual: {}\".format(obj[\"label\"]))\n",
    "            print(\"\")\n",
    "            i += 1\n",
    "            if (i == 2):\n",
    "                break\n",
    "\n",
    "test_dataset = dataset['test']\n",
    "find_wrong_prediction_scratch(test_dataset, logprior, loglikelihood, [0, 1], words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
